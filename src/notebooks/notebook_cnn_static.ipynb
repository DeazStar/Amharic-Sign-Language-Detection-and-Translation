{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in ./.venv/lib/python3.10/site-packages (2.18.0)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.10/site-packages (1.26.4)\n",
      "Requirement already satisfied: opencv-python in ./.venv/lib/python3.10/site-packages (4.11.0.86)\n",
      "Requirement already satisfied: keras in ./.venv/lib/python3.10/site-packages (3.8.0)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: mediapipe in ./.venv/lib/python3.10/site-packages (0.10.21)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement google.colab (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for google.colab\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install tensorflow numpy opencv-python keras pandas mediapipe google.colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 12:07:57.086559: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-28 12:07:57.217979: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-28 12:07:57.291551: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745831277.410131    7817 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745831277.439085    7817 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-28 12:07:57.630089: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf  # Import TensorFlow\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow import keras  # Import Keras\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: mediapipe\n",
      "Version: 0.10.21\n",
      "Summary: MediaPipe is the simplest way for researchers and developers to build world-class ML solutions and applications for mobile, edge, cloud and the web.\n",
      "Home-page: https://github.com/google/mediapipe\n",
      "Author: The MediaPipe Authors\n",
      "Author-email: mediapipe@google.com\n",
      "License: Apache 2.0\n",
      "Location: /home/deadstar/Documents/Amharic_Sign_Language/.venv/lib/python3.10/site-packages\n",
      "Requires: absl-py, attrs, flatbuffers, jax, jaxlib, matplotlib, numpy, opencv-contrib-python, protobuf, sentencepiece, sounddevice\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "! pip show mediapipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (495, 21, 2)\n",
      "y shape: (495, 33)\n",
      "[[-0.47997004  0.25145636]\n",
      " [-0.19652732  0.76834307]\n",
      " [ 0.00821337  0.81632661]\n",
      " [ 0.16550388  0.84515164]\n",
      " [ 0.32535475  1.        ]\n",
      " [ 0.04623465  0.23207582]\n",
      " [ 0.22882492  0.16423904]\n",
      " [ 0.35622632  0.13353405]\n",
      " [ 0.47801243  0.07816242]\n",
      " [ 0.01931254  0.0913957 ]\n",
      " [ 0.23102331  0.03317238]\n",
      " [ 0.38706751  0.02195604]\n",
      " [ 0.50439422 -0.00977705]\n",
      " [-0.01473724 -0.07880584]\n",
      " [ 0.20323355 -0.08741202]\n",
      " [ 0.34197281 -0.04574369]\n",
      " [ 0.46513729 -0.04080614]\n",
      " [-0.05080995 -0.24466568]\n",
      " [ 0.14510981 -0.21349707]\n",
      " [ 0.27171722 -0.15017593]\n",
      " [ 0.39161283 -0.10634443]]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "X_data = []\n",
    "y_data = []\n",
    "\n",
    "train_output_path = \"/home/deadstar/Documents/Amharic_Sign_Language/static_train_ouput.csv\"\n",
    "with open(train_output_path, 'r') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        landmark = np.array(ast.literal_eval(row[\"landmark\"]))  # shape: (21, 2)\n",
    "        encoding = np.array(ast.literal_eval(row[\"encoding\"]))  # shape: (10,)\n",
    "\n",
    "        X_data.append(landmark)\n",
    "        y_data.append(encoding)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X_train = np.array(X_data)  # shape: (num_samples, 21, 2)\n",
    "y_train = np.array(y_data)  # shape: (num_samples, 10)\n",
    "\n",
    "print(\"X shape:\", X_train.shape)\n",
    "print(\"y shape:\", y_train.shape)\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (165, 21, 2)\n",
      "y shape: (165, 33)\n",
      "[[[-0.12204395  0.93344829]\n",
      "  [ 0.29710872  0.87040636]\n",
      "  [ 0.51310943  0.6052178 ]\n",
      "  ...\n",
      "  [-0.04661332 -0.2568929 ]\n",
      "  [ 0.14328512 -0.37311921]\n",
      "  [ 0.31750526 -0.46723346]]\n",
      "\n",
      " [[-0.12174695  0.9641697 ]\n",
      "  [ 0.29638569  1.        ]\n",
      "  [ 0.51186075  0.77063004]\n",
      "  ...\n",
      "  [-0.04649988 -0.28505039]\n",
      "  [ 0.14293642 -0.36218331]\n",
      "  [ 0.31673259 -0.41961783]]\n",
      "\n",
      " [[-0.12174695  0.9641697 ]\n",
      "  [ 0.29638569  1.        ]\n",
      "  [ 0.51186075  0.77063004]\n",
      "  ...\n",
      "  [-0.04649988 -0.28505039]\n",
      "  [ 0.14293642 -0.36218331]\n",
      "  [ 0.31673259 -0.41961783]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1.         -0.21090132]\n",
      "  [-0.83176952  0.08403864]\n",
      "  [-0.50732722  0.25441988]\n",
      "  ...\n",
      "  [-0.14156184  0.15758331]\n",
      "  [-0.36166561  0.0952222 ]\n",
      "  [-0.42396509 -0.03580032]]\n",
      "\n",
      " [[-0.07231256  1.        ]\n",
      "  [ 0.07227786  0.69679067]\n",
      "  [ 0.33322657  0.45415879]\n",
      "  ...\n",
      "  [ 0.16498177  0.16020131]\n",
      "  [ 0.06377936  0.39241725]\n",
      "  [ 0.06872314  0.38211174]]\n",
      "\n",
      " [[-1.         -0.21090132]\n",
      "  [-0.83176952  0.08403864]\n",
      "  [-0.50732722  0.25441988]\n",
      "  ...\n",
      "  [-0.14156184  0.15758331]\n",
      "  [-0.36166561  0.0952222 ]\n",
      "  [-0.42396509 -0.03580032]]]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "X_data = []\n",
    "y_data = []\n",
    "\n",
    "test_output_path = \"/home/deadstar/Documents/Amharic_Sign_Language/static_test_ouput.csv\"\n",
    "with open(test_output_path, 'r') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        landmark = np.array(ast.literal_eval(row[\"landmark\"]))  # shape: (21, 2)\n",
    "        encoding = np.array(ast.literal_eval(row[\"encoding\"]))  # shape: (10,)\n",
    "\n",
    "        X_data.append(landmark)\n",
    "        y_data.append(encoding)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X_test = np.array(X_data)  # shape: (num_samples, 21, 2)\n",
    "y_test = np.array(y_data)  # shape: (num_samples, 10)\n",
    "\n",
    "print(\"X shape:\", X_test.shape)\n",
    "print(\"y shape:\", y_test.shape)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_6\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_6\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">704</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_18          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,352</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_19          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_6      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_20          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,089</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_12 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │           \u001b[38;5;34m704\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_18          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_12 (\u001b[38;5;33mActivation\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_6 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_12 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_13 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m12,352\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_19          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_6      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_20          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_13 (\u001b[38;5;33mActivation\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_13 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m33\u001b[0m)             │         \u001b[38;5;34m1,089\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">16,865</span> (65.88 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m16,865\u001b[0m (65.88 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">16,545</span> (64.63 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m16,545\u001b[0m (64.63 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> (1.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m320\u001b[0m (1.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.0200 - loss: 3.8370\n",
      "Epoch 2/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0471 - loss: 3.5501 \n",
      "Epoch 3/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0934 - loss: 3.1957\n",
      "Epoch 4/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1189 - loss: 3.1457 \n",
      "Epoch 5/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1567 - loss: 3.0326\n",
      "Epoch 6/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2175 - loss: 2.8818\n",
      "Epoch 7/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2324 - loss: 2.8865\n",
      "Epoch 8/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2286 - loss: 2.8179\n",
      "Epoch 9/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3037 - loss: 2.6340\n",
      "Epoch 10/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2774 - loss: 2.6294\n",
      "Epoch 11/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2948 - loss: 2.5545\n",
      "Epoch 12/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3200 - loss: 2.4915\n",
      "Epoch 13/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3130 - loss: 2.4337\n",
      "Epoch 14/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3695 - loss: 2.3329\n",
      "Epoch 15/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3940 - loss: 2.3207\n",
      "Epoch 16/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3726 - loss: 2.2713\n",
      "Epoch 17/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4232 - loss: 2.2267\n",
      "Epoch 18/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4520 - loss: 2.0819\n",
      "Epoch 19/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4599 - loss: 2.1446\n",
      "Epoch 20/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4628 - loss: 2.0840\n",
      "Epoch 21/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4345 - loss: 2.0121\n",
      "Epoch 22/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4953 - loss: 1.9367\n",
      "Epoch 23/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5480 - loss: 1.8403\n",
      "Epoch 24/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5468 - loss: 1.8000\n",
      "Epoch 25/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5322 - loss: 1.7015\n",
      "Epoch 26/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5409 - loss: 1.7556\n",
      "Epoch 27/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5515 - loss: 1.6589\n",
      "Epoch 28/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5398 - loss: 1.7726\n",
      "Epoch 29/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5891 - loss: 1.6155\n",
      "Epoch 30/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6069 - loss: 1.5671\n",
      "Epoch 31/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6372 - loss: 1.5259\n",
      "Epoch 32/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5983 - loss: 1.5406\n",
      "Epoch 33/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6363 - loss: 1.4642\n",
      "Epoch 34/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5703 - loss: 1.5457\n",
      "Epoch 35/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6314 - loss: 1.4113\n",
      "Epoch 36/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6046 - loss: 1.4929\n",
      "Epoch 37/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6325 - loss: 1.3702\n",
      "Epoch 38/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7446 - loss: 1.2630\n",
      "Epoch 39/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6906 - loss: 1.2375\n",
      "Epoch 40/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6650 - loss: 1.2007\n",
      "Epoch 41/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.7484 - loss: 1.1768\n",
      "Epoch 42/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.7117 - loss: 1.1628\n",
      "Epoch 43/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.7365 - loss: 1.1515\n",
      "Epoch 44/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7256 - loss: 1.1028\n",
      "Epoch 45/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.6948 - loss: 1.1246\n",
      "Epoch 46/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.7116 - loss: 1.0632\n",
      "Epoch 47/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.6806 - loss: 1.1753\n",
      "Epoch 48/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7571 - loss: 1.0017\n",
      "Epoch 49/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7468 - loss: 0.9667\n",
      "Epoch 50/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7364 - loss: 1.0335\n",
      "Epoch 51/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7533 - loss: 0.9715\n",
      "Epoch 52/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8000 - loss: 0.9427\n",
      "Epoch 53/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7656 - loss: 0.9875\n",
      "Epoch 54/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7434 - loss: 0.9719\n",
      "Epoch 55/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7432 - loss: 0.9057\n",
      "Epoch 56/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7751 - loss: 0.8691\n",
      "Epoch 57/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7669 - loss: 0.8642\n",
      "Epoch 58/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7480 - loss: 0.9309\n",
      "Epoch 59/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7702 - loss: 0.8318\n",
      "Epoch 60/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7703 - loss: 0.9002\n",
      "Epoch 61/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7976 - loss: 0.7667\n",
      "Epoch 62/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7870 - loss: 0.8099\n",
      "Epoch 63/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7979 - loss: 0.8003\n",
      "Epoch 64/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7793 - loss: 0.7910\n",
      "Epoch 65/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8030 - loss: 0.6848\n",
      "Epoch 66/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7738 - loss: 0.7811\n",
      "Epoch 67/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8396 - loss: 0.7143\n",
      "Epoch 68/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7866 - loss: 0.7852\n",
      "Epoch 69/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8736 - loss: 0.6258\n",
      "Epoch 70/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7959 - loss: 0.7119\n",
      "Epoch 71/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8138 - loss: 0.6584\n",
      "Epoch 72/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8215 - loss: 0.6625\n",
      "Epoch 73/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8290 - loss: 0.6497\n",
      "Epoch 74/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8263 - loss: 0.6798\n",
      "Epoch 75/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8428 - loss: 0.5943\n",
      "Epoch 76/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8302 - loss: 0.6772\n",
      "Epoch 77/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8581 - loss: 0.6195\n",
      "Epoch 78/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8367 - loss: 0.5988\n",
      "Epoch 79/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8490 - loss: 0.5947\n",
      "Epoch 80/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8269 - loss: 0.6030\n",
      "Epoch 81/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8910 - loss: 0.4834\n",
      "Epoch 82/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8685 - loss: 0.5747\n",
      "Epoch 83/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8628 - loss: 0.5603\n",
      "Epoch 84/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8691 - loss: 0.5538\n",
      "Epoch 85/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8367 - loss: 0.5761\n",
      "Epoch 86/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8715 - loss: 0.6003\n",
      "Epoch 87/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8290 - loss: 0.5801\n",
      "Epoch 88/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8500 - loss: 0.5595\n",
      "Epoch 89/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8982 - loss: 0.4451\n",
      "Epoch 90/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8519 - loss: 0.5126\n",
      "Epoch 91/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8754 - loss: 0.4687\n",
      "Epoch 92/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8699 - loss: 0.5452\n",
      "Epoch 93/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8485 - loss: 0.5270\n",
      "Epoch 94/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8810 - loss: 0.5184\n",
      "Epoch 95/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8370 - loss: 0.5691\n",
      "Epoch 96/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8694 - loss: 0.4966\n",
      "Epoch 97/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8655 - loss: 0.5168\n",
      "Epoch 98/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8417 - loss: 0.5454\n",
      "Epoch 99/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8659 - loss: 0.4618\n",
      "Epoch 100/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8976 - loss: 0.4164\n",
      "Epoch 101/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9039 - loss: 0.4232\n",
      "Epoch 102/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8900 - loss: 0.4655\n",
      "Epoch 103/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8954 - loss: 0.4177\n",
      "Epoch 104/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8750 - loss: 0.4230\n",
      "Epoch 105/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8942 - loss: 0.4209\n",
      "Epoch 106/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9143 - loss: 0.3615\n",
      "Epoch 107/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8973 - loss: 0.4419\n",
      "Epoch 108/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8739 - loss: 0.4470\n",
      "Epoch 109/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8527 - loss: 0.4556\n",
      "Epoch 110/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8948 - loss: 0.3704\n",
      "Epoch 111/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9137 - loss: 0.3480\n",
      "Epoch 112/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8774 - loss: 0.4143\n",
      "Epoch 113/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8860 - loss: 0.3907\n",
      "Epoch 114/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9053 - loss: 0.3690\n",
      "Epoch 115/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8797 - loss: 0.4058\n",
      "Epoch 116/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9120 - loss: 0.3499\n",
      "Epoch 117/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9078 - loss: 0.3900\n",
      "Epoch 118/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8916 - loss: 0.3861\n",
      "Epoch 119/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8931 - loss: 0.3632\n",
      "Epoch 120/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9314 - loss: 0.3289\n",
      "Epoch 121/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9338 - loss: 0.3199\n",
      "Epoch 122/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9092 - loss: 0.2999\n",
      "Epoch 123/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9095 - loss: 0.3460\n",
      "Epoch 124/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9154 - loss: 0.2974\n",
      "Epoch 125/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9009 - loss: 0.3111\n",
      "Epoch 126/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8843 - loss: 0.3726\n",
      "Epoch 127/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9224 - loss: 0.3025\n",
      "Epoch 128/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9361 - loss: 0.2962\n",
      "Epoch 129/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9279 - loss: 0.3217\n",
      "Epoch 130/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9134 - loss: 0.3099\n",
      "Epoch 131/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8824 - loss: 0.3554\n",
      "Epoch 132/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9129 - loss: 0.3308\n",
      "Epoch 133/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9155 - loss: 0.3400\n",
      "Epoch 134/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9285 - loss: 0.3103\n",
      "Epoch 135/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9444 - loss: 0.2491\n",
      "Epoch 136/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9115 - loss: 0.3483\n",
      "Epoch 137/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9512 - loss: 0.2731\n",
      "Epoch 138/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9056 - loss: 0.3271\n",
      "Epoch 139/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9228 - loss: 0.3290\n",
      "Epoch 140/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9134 - loss: 0.3623\n",
      "Epoch 141/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9228 - loss: 0.2932\n",
      "Epoch 142/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9351 - loss: 0.2909\n",
      "Epoch 143/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9323 - loss: 0.2726\n",
      "Epoch 144/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9241 - loss: 0.3097\n",
      "Epoch 145/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8896 - loss: 0.3158\n",
      "Epoch 146/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9341 - loss: 0.2483\n",
      "Epoch 147/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8933 - loss: 0.2882\n",
      "Epoch 148/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9427 - loss: 0.2785\n",
      "Epoch 149/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9432 - loss: 0.2075\n",
      "Epoch 150/150\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9299 - loss: 0.2530\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dropout, GlobalAveragePooling1D, Dense, BatchNormalization, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class CNN:\n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        self.num_classes = num_classes\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "    def build_model(self):\n",
    "            \n",
    "        model = Sequential([\n",
    "        # Input Layer\n",
    "        tf.keras.layers.Input(shape=input_shape),\n",
    "\n",
    "        # Convolutional Blocks (Reduced Complexity)\n",
    "        Conv1D(64, kernel_size=5, padding='same', activation=None),  # Reduced filters\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        Conv1D(64, kernel_size=3, padding='same', activation='relu'),  # Reduced filters\n",
    "        BatchNormalization(),\n",
    "        GlobalAveragePooling1D(),  # Or GlobalMaxPooling1D()\n",
    "\n",
    "        # Dense Layers (Reduced Complexity)\n",
    "        Dense(32, activation=None),  # Reduced units\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Dense(num_classes, activation='softmax')  # Output layer\n",
    "            ])\n",
    "\n",
    "        model.compile(optimizer=Adam(learning_rate=0.0005),  # Adjusted learning rate\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "\n",
    "# Model Creation\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])  # (21, 2)\n",
    "num_classes = 33\n",
    "model = CNN(input_shape, num_classes).build_model()\n",
    "model.summary()\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy',  # Changed to val_accuracy\n",
    "                                 patience=30,\n",
    "                                 restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=150,  # Increased epochs\n",
    "                    batch_size=16, callbacks=[early_stopping])\n",
    "\n",
    "# (Optional) Plotting training history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='train_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"data:0\", shape=(1, 42), dtype=float32). Expected shape (None, 21, 2), but input has incompatible shape (1, 42)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(1, 42), dtype=float32)\n  • training=False\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(pred)\n",
      "File \u001b[0;32m~/Documents/Amharic_Sign_Language/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Documents/Amharic_Sign_Language/.venv/lib/python3.10/site-packages/keras/src/models/functional.py:272\u001b[0m, in \u001b[0;36mFunctional._adjust_input_rank\u001b[0;34m(self, flat_inputs)\u001b[0m\n\u001b[1;32m    270\u001b[0m             adjusted\u001b[38;5;241m.\u001b[39mappend(ops\u001b[38;5;241m.\u001b[39mexpand_dims(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    271\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 272\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    273\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input shape for input \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mref_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but input has incompatible shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    275\u001b[0m     )\n\u001b[1;32m    276\u001b[0m \u001b[38;5;66;03m# Add back metadata.\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(flat_inputs)):\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"data:0\", shape=(1, 42), dtype=float32). Expected shape (None, 21, 2), but input has incompatible shape (1, 42)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(1, 42), dtype=float32)\n  • training=False\n  • mask=None"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Normalization:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        mcp_indices: Indcies of hte MCP joints\n",
    "            5: Represent the Index finger\n",
    "            9: Represent the Middle finger\n",
    "            13: Represent the Ring finger\n",
    "            17: Represent the Pinky finger\n",
    "        \"\"\"\n",
    "        self.mcp_indices = [5, 9, 13, 17]\n",
    "\n",
    "    def bbox_normalize_landmarks(self, landmarks, roi, image_width, image_height):\n",
    "        \"\"\"\n",
    "        Normalizes landmarks relative to the bounding box (roi).\n",
    "\n",
    "        Args:\n",
    "            landmarks: List of landmarks with normalized coordinates (x, y) relative to full image.\n",
    "            roi: Tuple (roi_x_min, roi_y_min, roi_x_max, roi_y_max) in pixel coordinates.\n",
    "\n",
    "        Returns:\n",
    "            landmarks_normalized: Landmarks normalized to [0,1] within the ROI box.\n",
    "        \"\"\"\n",
    "        roi_x_min, roi_y_min, roi_x_max, roi_y_max = roi\n",
    "\n",
    "        width = roi_x_max - roi_x_min\n",
    "        height = roi_y_max - roi_y_min\n",
    "\n",
    "        landmarks_px = np.array([[lm.x, lm.y] for lm in landmarks])\n",
    "\n",
    "        landmarks_px[:, 0] = (landmarks_px[:, 0] * image_width - roi_x_min) / width\n",
    "        landmarks_px[:, 1] = (landmarks_px[:, 1] * image_height - roi_y_min) / height\n",
    "\n",
    "        return landmarks_px\n",
    "\n",
    "    def center_hand_landmark(self, landmarks):\n",
    "        \"\"\"\n",
    "        Centers the hand landmarks around the palm center.\n",
    "        \n",
    "        Args:\n",
    "            landmarks: The hand landmarks (numpy array).\n",
    "        \"\"\"\n",
    "        mcp_joints = landmarks[self.mcp_indices]\n",
    "        center = np.mean(mcp_joints, axis=0)\n",
    "        centered = landmarks - center\n",
    "        return centered\n",
    "    \n",
    "    def scale_hand_landmark(self, landmarks):\n",
    "        \"\"\"\n",
    "        Scales the hand landmarks based on the distance from the MCP joints to the center.\n",
    "        \n",
    "        Args:\n",
    "            landmarks: The hand landmarks (numpy array).\n",
    "        \"\"\"\n",
    "        mcp_joints = landmarks[self.mcp_indices]\n",
    "        center = np.mean(mcp_joints, axis=0)\n",
    "        dists = np.linalg.norm(mcp_joints - center, axis=1)\n",
    "        scale = np.mean(dists)\n",
    "        if scale < 1e-6:\n",
    "            scale = 1e-6\n",
    "        return landmarks / scale\n",
    "    \n",
    "    def rotate_hand_landmark(self, landmarks):\n",
    "        \"\"\"\n",
    "        Rotates the hand landmarks to align the index finger (MCP joint) upward.\n",
    "        \n",
    "        Args:\n",
    "            landmarks: The hand landmarks (numpy array).\n",
    "        \"\"\"\n",
    "        # Align L9 (index MCP) upward\n",
    "        vec = landmarks[9]\n",
    "        reference = np.array([0, -1])\n",
    "        dot = np.dot(vec, reference)\n",
    "        det = vec[0] * reference[1] - vec[1] * reference[0]\n",
    "        theta = np.arctan2(det, dot)\n",
    "    \n",
    "        cos_t = np.cos(-theta)\n",
    "        sin_t = np.sin(-theta)\n",
    "        R = np.array([\n",
    "            [cos_t, -sin_t],\n",
    "            [sin_t,  cos_t]\n",
    "        ])\n",
    "    \n",
    "        return landmarks @ R.T\n",
    "    \n",
    "    def normalize_hand_landmarks(self, landmarks):\n",
    "        \"\"\"\n",
    "        Normalizes the hand landmarks by centering, scaling, and rotating.\n",
    "        \n",
    "        Args:\n",
    "            landmarks: The hand landmarks (numpy array).\n",
    "        \"\"\"\n",
    "        centered_landmarks = self.center_hand_landmark(landmarks)\n",
    "        scaled_landmarks = self.scale_hand_landmark(centered_landmarks)\n",
    "        rotated_landmarks = self.rotate_hand_landmark(scaled_landmarks)\n",
    "        \n",
    "        # New: normalize to [-1, 1] range\n",
    "        max_val = np.max(np.abs(rotated_landmarks))\n",
    "        if max_val > 0:\n",
    "            rotated_landmarks = rotated_landmarks / max_val\n",
    "\n",
    "        return rotated_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 09:38:11.022133: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-30 09:38:11.029225: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-30 09:38:11.045258: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745995091.068998   25665 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745995091.076254   25665 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-30 09:38:11.099878: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "class MediaPipeWrapper:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the MediaPipe wrapper for hand detection.\n",
    "        \"\"\"\n",
    "        self.base_option = python.BaseOptions(model_asset_path='./hand_landmarker.task')\n",
    "        self.option = vision.HandLandmarkerOptions(base_options=self.base_option,\n",
    "                                                   num_hands=2)\n",
    "        self.detector = vision.HandLandmarker.create_from_options(self.option)\n",
    "        \n",
    "    def detect_hands(self, frame):\n",
    "        \"\"\"\n",
    "        Detects hands in the image using MediaPipe HandLandmarker (Tasks API).\n",
    "        \n",
    "        Args:\n",
    "            frame: Input image (numpy array, BGR).\n",
    "        \"\"\"\n",
    "        H, W, _ = frame.shape\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        result = self.detector.detect(mp_image)\n",
    "        return result\n",
    "\n",
    "    def extract_hand_roi(self, frame):\n",
    "        \"\"\"\n",
    "        Extracts the cropped hand region and landmarks from the input frame.\n",
    "\n",
    "        Args:\n",
    "            frame: Input image (numpy array, BGR).\n",
    "        Returns:\n",
    "            hand_roi: Cropped hand image (numpy array) or None if no hand detected.\n",
    "            hand_landmarks: List of landmarks or None if no hand detected.\n",
    "        \"\"\"\n",
    "        result = self.detect_hands(frame)\n",
    "\n",
    "        H, W, _ = frame.shape\n",
    "\n",
    "        if not result.hand_landmarks:\n",
    "            return None, None, None  # No hands detected\n",
    "\n",
    "        # Assume first detected hand\n",
    "        hand_landmarks = result.hand_landmarks[0]\n",
    "\n",
    "        x_coords = [lm.x for lm in hand_landmarks]\n",
    "        y_coords = [lm.y for lm in hand_landmarks]\n",
    "\n",
    "        x_min = int(min(x_coords) * W)\n",
    "        y_min = int(min(y_coords) * H)\n",
    "        x_max = int(max(x_coords) * W)\n",
    "        y_max = int(max(y_coords) * H)\n",
    "\n",
    "        margin_x = int(0.3 * (x_max - x_min))\n",
    "        margin_y = int(0.3 * (y_max - y_min))\n",
    "\n",
    "        roi_x_min = max(0, x_min - margin_x)\n",
    "        roi_y_min = max(0, y_min - margin_y)\n",
    "        roi_x_max = min(W, x_max + margin_x)\n",
    "        roi_y_max = min(H, y_max + margin_y)\n",
    "\n",
    "        hand_roi = frame[roi_y_min:roi_y_max, roi_x_min:roi_x_max]\n",
    "\n",
    "        roi = (roi_x_min, roi_y_min, roi_x_max, roi_y_max)\n",
    "\n",
    "        return hand_roi, hand_landmarks, roi\n",
    "\n",
    "# # === USAGE ===\n",
    "# wrapper = MediaPipeWrapper()\n",
    "# # \n",
    "# # # Load your image\n",
    "# frame = cv2.imread('./cropped_hand.jpg')  # 👈\n",
    "# print(frame.shape)\n",
    "# landmark = wrapper.detect_hands(frame)\n",
    "# print(landmark)\n",
    "# # Resize image before processing (important!)\n",
    "# #frame = cv2.resize(frame, (640, 480))\n",
    "# #\n",
    "# #cv2.imshow('Original Image', frame)\n",
    "# #cv2.waitKey(0)\n",
    "# #cv2.destroyAllWindows()\n",
    "# #'''\n",
    "# \n",
    "# cropped_hand = wrapper.extract_hand_roi(frame)\n",
    "# \n",
    "# # Define your desired window width and height\n",
    "# desired_width = 800\n",
    "# desired_height = 600\n",
    "# \n",
    "# if cropped_hand is not None:\n",
    "#     save_path = 'cropped_hand.jpg'  # 👈 where you want to save\n",
    "#     cv2.imwrite(save_path, cropped_hand)\n",
    "# else:\n",
    "#     print(\"No hand detected.\")\n",
    "# \n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os \n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "\n",
    "class ImagePreProcessor:\n",
    "    def __init__(self, num_classes):\n",
    "        \"\"\"\n",
    "        Initializes the ImagePreProcessor with  normalziation object\n",
    "        \"\"\"\n",
    "\n",
    "        self.normalization = Normalization()\n",
    "        self.mediapipewrapper = MediaPipeWrapper()\n",
    "        self.hand_visualizer = HandVisualizer()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def load_labels(self, label_path):\n",
    "        \"\"\"\n",
    "        Load labels from the label file.\n",
    "        \"\"\"\n",
    "        df = pd.read_csv(label_path)\n",
    "        return dict(zip(df['filename'], df['class']))\\\n",
    "\n",
    "    def one_hot_encoding(self, y):\n",
    "        \"\"\"\n",
    "        Convert labels to one-hot encoding.\n",
    "        \n",
    "        Args:\n",
    "            y: List of labels to be converted.\n",
    "        \"\"\"\n",
    "\n",
    "        sorted_labels = sorted(set(y)) # unique soted labels\n",
    "        label_to_index = {label: idx for idx, label in enumerate(sorted_labels)}\n",
    "        indices = [label_to_index[label] for label in y]\n",
    "        return (tf.one_hot(indices, depth=self.num_classes), y)\n",
    "    \n",
    "    def load_single_image(self, frame, show=False):\n",
    "        # frame = cv2.imread(file)\n",
    "        \n",
    "        H, W, _ = frame.shape\n",
    "        \n",
    "        hand_roi, hand_landmarks, roi = self.mediapipewrapper.extract_hand_roi(frame)\n",
    "\n",
    "        if hand_roi is None:\n",
    "            # raise ValueError(f\"Hand not detected in image: {frame}\")\n",
    "            return None, None , None\n",
    "\n",
    "        landmark_px = self.normalization.bbox_normalize_landmarks(hand_landmarks, roi, W, H)\n",
    "        \n",
    "        if show:\n",
    "            annotated_image = self.hand_visualizer.draw_landmarks_on_image(hand_roi, hand_landmarks)\n",
    "            self.hand_visualizer.draw_hand(annotated_image)\n",
    "        return landmark_px\n",
    "\n",
    "    def load_multiple_images(self, image_path, lable_path):\n",
    "        \"\"\"\n",
    "        Load images from a direcotry\n",
    "        Extract landmarks from each image\n",
    "        Normalize the landmarks\n",
    "        Return the normalized landmarks and labels\n",
    "        \n",
    "        Args:\n",
    "            image_path: Path to the image file.\n",
    "            label_path: Path to the label file.\n",
    "        \"\"\"\n",
    "        \n",
    "        X = []\n",
    "        y = []\n",
    "        labels = self.load_labels(lable_path)\n",
    "        \n",
    "        files = sorted([\n",
    "            f for f in os.listdir(image_path) \n",
    "            if f.lower().endswith(('.jpg', '.jpeg', '.png')) and not f.startswith('invert')\n",
    "        ])\n",
    "        \n",
    "        for file in files:\n",
    "            file_path = os.path.join(image_path, file)\n",
    "            \n",
    "            normalized_landmarks = self.load_single_image(file_path)\n",
    "            \n",
    "            if normalized_landmarks is not None:\n",
    "                X.append(normalized_landmarks)\n",
    "                label =  labels.get(file, -1)\n",
    "                if label == -1:\n",
    "                    print(f\"Warning: Missing label for {file}\")\n",
    "                y.append(label)\n",
    "            else:\n",
    "                print(f\"Warning: Skipping {file} due to missing hand landmarks\")\n",
    "\n",
    "        y = self.one_hot_encoding(np.array(y))\n",
    "        \n",
    "        return np.array(X), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Save entire model (architecture + weights)\n",
    "# model.save(\"model-cnn-normalized-augmented-84.keras\")\n",
    "#Load model\n",
    "#from tensorflow.keras.models import load_model\n",
    "# model = load_model(\"model-cnn-full-roi-extra-augmented.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7635 - loss: 0.7348  \n",
      "Test accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class HandVisualizer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def draw_landmarks_on_image(self, image, detection_result):\n",
    "        \"\"\"\n",
    "        Draws hand landmarks on the image.\n",
    "        \n",
    "        Args:\n",
    "            image: The input image (numpy array).\n",
    "            detection_result: The result of hand detection containing landmarks.\n",
    "        \"\"\"\n",
    "        # Convert the image to an OpenCV format\n",
    "        annotated_image = image.copy()\n",
    "\n",
    "        for hand_landmarks in detection_result.hand_landmarks:\n",
    "            for landmark in hand_landmarks:\n",
    "                x, y = int(landmark.x * image.shape[1]), int(landmark.y * image.shape[0])\n",
    "                cv2.circle(annotated_image, (x, y), 5, (0, 255, 0), -1)\n",
    "\n",
    "        return annotated_image\n",
    "    \n",
    "    def daraw_hand(self, hand_roi):\n",
    "        \"\"\"\n",
    "        Draws the Region of Interest (ROI) for the hand.\n",
    "        \n",
    "        Args:\n",
    "            hand_roi: The Region of Interest (ROI) for the hand (numpy array).\n",
    "        \"\"\"\n",
    "        # Convert to RGB for displaying in matplotlib\n",
    "        hand_roi_rgb = cv2.cvtColor(hand_roi, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Display the ROI in the notebook\n",
    "        plt.imshow(hand_roi_rgb)\n",
    "        plt.axis('off')  # Hide axis\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1745818044.386545   14328 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1745818044.389806   71743 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 25.0.2), renderer: Mesa Intel(R) UHD Graphics 620 (WHL GT2)\n",
      "W0000 00:00:1745818044.451713   71745 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745818044.474159   71744 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📸 Camera is ready. Press Enter to capture image...\n",
      "✅ Image captured and saved at: Amharic_Sign_Language/le.jpg\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step\n",
      "🔮 Prediction: [[1.08582368e-04 1.68731663e-06 7.86396027e-01 1.87537214e-03\n",
      "  2.39480432e-05 1.01315655e-01 5.73692610e-04 1.17168740e-04\n",
      "  6.86241692e-05 1.04504143e-04 2.15089985e-05 1.17766904e-03\n",
      "  1.45671729e-05 4.96191351e-05 7.88451880e-02 5.71888495e-06\n",
      "  8.64090271e-06 3.04547393e-06 1.97435915e-02 5.77406863e-05\n",
      "  5.49449614e-05 4.36882874e-05 8.82147346e-03 1.41042256e-05\n",
      "  1.84372027e-06 1.87384903e-05 4.26145334e-06 5.85838052e-06\n",
      "  4.49872387e-05 2.76939711e-04 3.32086529e-06 1.00781770e-04\n",
      "  9.66092339e-05]]\n",
      "✅ Predicted Class Index: 2\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import time\n",
    "\n",
    "image_procssor = ImagePreProcessor(num_classes=33)\n",
    "normalizer = Normalization()\n",
    "\n",
    "def capture_image_from_camera(save_path='Amharic_Sign_Language/le.jpg'):\n",
    "    cap = cv2.VideoCapture(0)  # Open default camera\n",
    "\n",
    "    print(\"📸 Camera is ready. Press Enter to capture image...\")\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"❌ Failed to grab frame.\")\n",
    "            break\n",
    "\n",
    "        cv2.imshow(\"Live Camera - Press Enter to Capture\", frame)\n",
    "\n",
    "        key = cv2.waitKey(1)\n",
    "        if key == 13:  # Enter key\n",
    "            cv2.imwrite(save_path, frame)\n",
    "            print(f\"✅ Image captured and saved at: {save_path}\")\n",
    "            break\n",
    "        elif key == 27:  # Esc key to exit\n",
    "            print(\"❌ Capture cancelled.\")\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return save_path\n",
    "\n",
    "\n",
    "# --- Main live test flow ---\n",
    "\n",
    "image_path = capture_image_from_camera()\n",
    "\n",
    "if os.path.exists(image_path):\n",
    "    feat = image_procssor.load_single_image(image_path,False)\n",
    "    \n",
    "\n",
    "    if feat is not None:\n",
    "        # Normalize the original sample\n",
    "        normalized_original = normalizer.normalize_hand_landmarks(feat)\n",
    "        normalized_original = normalized_original.reshape(1, 21, 2)\n",
    "        prediction = model.predict(normalized_original)\n",
    "        print(\"🔮 Prediction:\", prediction)\n",
    "        print(\"✅ Predicted Class Index:\", prediction.argmax())\n",
    "    else:\n",
    "        print(\"❌ No hand detected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Save entire model (architecture + weights)\n",
    "# model.save(\"model-cnn-full-roi-extra-augmented.h5\")\n",
    "model.save(\"model-cnn-full-roi-extra-augmented.keras\")\n",
    "#Load model\n",
    "# from tensorflow.keras.models import load_model\n",
    "# model = load_model(\"model-cnn-normalized-augmented.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 02:21:42.610775: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "/home/deadstar/Documents/Amharic_Sign_Language/.venv/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 16 variables whereas the saved optimizer has 30 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpnksf1rq2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpnksf1rq2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/tmp/tmpnksf1rq2'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 21, 2), dtype=tf.float32, name='input_layer_6')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 33), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  140668284795744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140668285013856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140668285024416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140668285157792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140668285019840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140668285022128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140668285172752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140668285173456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140668285168000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140668285371136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140668285372192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140668285373776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140668285380992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140668285381696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140668285374480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140668285385392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140668285385744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140668233089440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140668233104224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140668233102464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1745968905.942712   55570 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1745968905.942758   55570 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2025-04-30 02:21:45.943819: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpnksf1rq2\n",
      "2025-04-30 02:21:45.947000: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2025-04-30 02:21:45.947132: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmpnksf1rq2\n",
      "I0000 00:00:1745968905.976007   55570 mlir_graph_optimization_pass.cc:401] MLIR V1 optimization pass is not enabled\n",
      "2025-04-30 02:21:45.984072: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2025-04-30 02:21:46.204256: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmpnksf1rq2\n",
      "2025-04-30 02:21:46.277645: I tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 333830 microseconds.\n",
      "2025-04-30 02:21:46.391561: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the Keras model from file\n",
    "model = tf.keras.models.load_model('./model-cnn-normalized-augmented-84.keras')\n",
    "\n",
    "# Convert to TensorFlow Lite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the converted model to a .tflite file\n",
    "with open('model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 09:38:35.179126: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "/home/deadstar/Documents/Amharic_Sign_Language/.venv/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 16 variables whereas the saved optimizer has 30 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "# Real time part\n",
    "# Load the Keras model from file\n",
    "model = tf.keras.models.load_model('./model-cnn-normalized-augmented-84.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1745995117.470309   25665 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1745995117.475007   26020 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 25.0.4), renderer: Mesa Intel(R) UHD Graphics 620 (WHL GT2)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1745995117.513480   26022 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745995117.546472   26026 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "Warning: Ignoring XDG_SESSION_TYPE=wayland on Gnome. Use QT_QPA_PLATFORM=wayland to run on Wayland anyway.\n",
      "W0000 00:00:1745995118.097017   26028 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 23\u001b[0m\n\u001b[1;32m     18\u001b[0m feat \u001b[38;5;241m=\u001b[39m image_procssor\u001b[38;5;241m.\u001b[39mload_single_image(frame, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m feat \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# Normalize the original sample\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m     normalized_original \u001b[38;5;241m=\u001b[39m \u001b[43mnormalizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize_hand_landmarks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     normalized_original \u001b[38;5;241m=\u001b[39m normalized_original\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m21\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     25\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(normalized_original)\n",
      "Cell \u001b[0;32mIn[1], line 94\u001b[0m, in \u001b[0;36mNormalization.normalize_hand_landmarks\u001b[0;34m(self, landmarks)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mnormalize_hand_landmarks\u001b[39m(\u001b[38;5;28mself\u001b[39m, landmarks):\n\u001b[1;32m     88\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;124;03m    Normalizes the hand landmarks by centering, scaling, and rotating.\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;124;03m        landmarks: The hand landmarks (numpy array).\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m     centered_landmarks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcenter_hand_landmark\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlandmarks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m     scaled_landmarks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_hand_landmark(centered_landmarks)\n\u001b[1;32m     96\u001b[0m     rotated_landmarks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotate_hand_landmark(scaled_landmarks)\n",
      "Cell \u001b[0;32mIn[1], line 44\u001b[0m, in \u001b[0;36mNormalization.center_hand_landmark\u001b[0;34m(self, landmarks)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcenter_hand_landmark\u001b[39m(\u001b[38;5;28mself\u001b[39m, landmarks):\n\u001b[1;32m     38\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    Centers the hand landmarks around the palm center.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m        landmarks: The hand landmarks (numpy array).\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m     mcp_joints \u001b[38;5;241m=\u001b[39m \u001b[43mlandmarks\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmcp_indices\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     45\u001b[0m     center \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(mcp_joints, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     46\u001b[0m     centered \u001b[38;5;241m=\u001b[39m landmarks \u001b[38;5;241m-\u001b[39m center\n",
      "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not list"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0)  # 0 = default camera\n",
    "image_procssor = ImagePreProcessor(num_classes=33)\n",
    "normalizer = Normalization()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Show the frame\n",
    "    cv2.imshow(\"Webcam Feed\", frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "    \n",
    "    feat = image_procssor.load_single_image(frame, False)\n",
    "    print(feat)\n",
    "    \n",
    "    \n",
    "    if feat is not None:\n",
    "        # Normalize the original sample\n",
    "        normalized_original = normalizer.normalize_hand_landmarks(feat)\n",
    "        normalized_original = normalized_original.reshape(1, 21, 2)\n",
    "        prediction = model.predict(normalized_original)\n",
    "        print(\"🔮 Prediction:\", prediction)\n",
    "        print(\"✅ Predicted Class Index:\", prediction.argmax())\n",
    "    else:\n",
    "        print(\"❌ No hand detected.\")\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
