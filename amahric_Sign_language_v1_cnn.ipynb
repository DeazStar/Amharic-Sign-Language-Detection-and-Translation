{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in ./.venv/lib/python3.10/site-packages (2.18.0)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.10/site-packages (1.26.4)\n",
      "Requirement already satisfied: opencv-python in ./.venv/lib/python3.10/site-packages (4.11.0.86)\n",
      "Requirement already satisfied: keras in ./.venv/lib/python3.10/site-packages (3.8.0)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in ./.venv/lib/python3.10/site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./.venv/lib/python3.10/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in ./.venv/lib/python3.10/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in ./.venv/lib/python3.10/site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in ./.venv/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in ./.venv/lib/python3.10/site-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: six>=1.12.0 in ./.venv/lib/python3.10/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in ./.venv/lib/python3.10/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./.venv/lib/python3.10/site-packages (from tensorflow) (1.70.0)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./.venv/lib/python3.10/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in ./.venv/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./.venv/lib/python3.10/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in ./.venv/lib/python3.10/site-packages (from tensorflow) (25.1.24)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in ./.venv/lib/python3.10/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in ./.venv/lib/python3.10/site-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in ./.venv/lib/python3.10/site-packages (from tensorflow) (4.25.6)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./.venv/lib/python3.10/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.10/site-packages (from tensorflow) (65.5.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in ./.venv/lib/python3.10/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.10/site-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: optree in ./.venv/lib/python3.10/site-packages (from keras) (0.14.0)\n",
      "Requirement already satisfied: namex in ./.venv/lib/python3.10/site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: rich in ./.venv/lib/python3.10/site-packages (from keras) (13.9.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.10/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.10/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./.venv/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./.venv/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./.venv/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./.venv/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.10/site-packages (from rich->keras) (2.19.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.10/site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./.venv/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install tensorflow numpy opencv-python keras pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf  # Import TensorFlow\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow import keras  # Import Keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "class VideoProcessor:\n",
    "  def __init__(self, video_path, label_path, sequence_length=21):\n",
    "    self.IMAGE_HEIGHT = 224\n",
    "    self.IMAGE_WIDTH = 224\n",
    "    self.sequence_length = sequence_length\n",
    "    self.video_path = video_path\n",
    "    self.label_path = label_path\n",
    "    self.frames = []\n",
    "    self.labels = {}\n",
    "    \n",
    "    self.load_labels()\n",
    "    \n",
    "  def load_labels(self):\n",
    "    \"\"\"Load labels from a csv file\"\"\"\n",
    "    df = pd.read_csv(self.label_path)\n",
    "    # map a file name with the label\n",
    "    self.labels = dict(zip(df['filename'], df['class']))\n",
    "    \n",
    "  def one_hot_encoding(self, y):\n",
    "    \"\"\"One-hot encoding of the labels\"\"\"\n",
    "    # Get unique sorted labels\n",
    "    sorted_labels = sorted(set(y))  # unique sorted labels\n",
    "    print(sorted_labels)\n",
    "    \n",
    "    # Map labels to indices\n",
    "    label_to_index = {label: idx for idx, label in enumerate(sorted_labels)}\n",
    "    \n",
    "    # Convert labels to indices\n",
    "    indices = [label_to_index[label] for label in y]\n",
    "    \n",
    "    # One-hot encode the labels\n",
    "    \n",
    "    y_one_hot = tf.one_hot(indices, depth=len(sorted_labels))\n",
    "    \n",
    "    return y_one_hot\n",
    "  \n",
    "  def load_single_image(self, file):\n",
    "    img = cv2.imread(file)\n",
    "    print(img)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "    img = cv2.resize(img, (self.IMAGE_HEIGHT, self.IMAGE_WIDTH))\n",
    "    img = tf.keras.applications.resnet50.preprocess_input(img)\n",
    "\n",
    "    return img\n",
    "\n",
    "  def load_frames(self):\n",
    "    '''\n",
    "      load from the file (images)\n",
    "      resize the images to 224x224\n",
    "      normalize the rgb to 0-1\n",
    "      return the frames\n",
    "    '''\n",
    "    X, y = [], []\n",
    "        \n",
    "    video_frame_files = sorted([\n",
    "        f for f in os.listdir(self.video_path) \n",
    "        if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
    "    ])\n",
    "\n",
    "    for files in video_frame_files:\n",
    "      # for now skip the invert images\n",
    "      if files.startswith('invert'):\n",
    "        continue\n",
    "      img = cv2.imread(os.path.join(self.video_path, files))\n",
    "      \n",
    "      if img is None:\n",
    "        print(\"Warning: Could not read image file\", files)\n",
    "        \n",
    "      img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "      img = cv2.resize(img, (self.IMAGE_HEIGHT, self.IMAGE_WIDTH))\n",
    "      img = tf.keras.applications.resnet50.preprocess_input(img) \n",
    "      \n",
    "      # Append the image frame to the list\n",
    "      X.append(np.array(img))\n",
    "      print(\"File: \", files)\n",
    "      if self.labels.get(files, -1) == -1:\n",
    "        print(\"Warning: Could not find label for image file\", files)\n",
    "      y.append(self.labels.get(files, -1))\n",
    "        # Append the label corresponding to the file\n",
    "      \n",
    "    # One-hot encode the labels after collecting all the images\n",
    "    y = self.one_hot_encoding(np.array(y))\n",
    "    \n",
    "    # Return the frames and corresponding labels\n",
    "    return np.array(X), y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor:\n",
    "    def __init__(self, image_height, image_width):\n",
    "        self.image_height = image_height\n",
    "        self.image_width = image_width\n",
    "        self.CHANNEL = 3\n",
    "    \n",
    "    def flatten_frames(self, frames):\n",
    "        '''\n",
    "        Flatten the images into individual frames\n",
    "        '''\n",
    "        # Reshape frames to process through CNN: (1722, 224, 224, 3)\n",
    "        frames_reshaped = frames.reshape(-1, self.image_height, self.image_width, self.CHANNEL)\n",
    "        return frames_reshaped\n",
    "\n",
    "    def extract_features(self, frames):\n",
    "        num_samples, h, w, c = frames.shape  # Shape of X (1722, 224, 224, 3)\n",
    "        frames = self.flatten_frames(frames)  # Flatten to (1722, 224, 224, 3)\n",
    "    \n",
    "        '''\n",
    "        Extract features using ResNet50 pre-trained CNN model with transfer learning\n",
    "        '''\n",
    "        base_model = tf.keras.applications.ResNet50(weights='imagenet', \n",
    "                                                    include_top=False, \n",
    "                                                    input_shape=(self.image_height, self.image_width, self.CHANNEL))\n",
    "    \n",
    "        # Freeze the base model layers\n",
    "        base_model.trainable = False\n",
    "    \n",
    "        # Apply Global Average Pooling\n",
    "        x = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "        \n",
    "        # Create a new model with ResNet50 as base and Global Average Pooling\n",
    "        cnn_model = tf.keras.Model(inputs=base_model.input, outputs=x)\n",
    "    \n",
    "        # Extract image features (Shape will be: (1722, 2048) after pooling)\n",
    "        features = cnn_model.predict(frames)\n",
    "    \n",
    "        # The features are already flattened for use in the Dense layers\n",
    "        return features\n",
    "    \n",
    "    def extract_single_image_feature(self, image):\n",
    "        \"\"\"\n",
    "        Extract features from a single image using ResNet50.\n",
    "        \"\"\"\n",
    "    \n",
    "        # Ensure the image has the correct shape (224, 224, 3)\n",
    "        if image.shape != (self.image_height, self.image_width, self.CHANNEL):\n",
    "            raise ValueError(f\"Expected image shape ({self.image_height}, {self.image_width}, {self.CHANNEL}), but got {image.shape}\")\n",
    "    \n",
    "        # Expand dimensions to match batch format (1, 224, 224, 3)\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "    \n",
    "        '''\n",
    "        Extract features using ResNet50 pre-trained CNN model with transfer learning\n",
    "        '''\n",
    "        base_model = tf.keras.applications.ResNet50(weights='imagenet', \n",
    "                                                    include_top=False, \n",
    "                                                    input_shape=(self.image_height, self.image_width, self.CHANNEL))\n",
    "    \n",
    "        # Freeze the base model layers\n",
    "        base_model.trainable = False\n",
    "    \n",
    "        # Apply Global Average Pooling\n",
    "        x = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "    \n",
    "        # Create a new model with ResNet50 as base and Global Average Pooling\n",
    "        cnn_model = tf.keras.Model(inputs=base_model.input, outputs=x)\n",
    "    \n",
    "        # Extract image features (Shape will be: (1, 2048) after pooling)\n",
    "        features = cnn_model.predict(image)\n",
    "    \n",
    "        # Remove batch dimension (1, 2048) → (2048,)\n",
    "        return features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['che', 'gne', 'ha', 'he', 'hhe', 'ke', 'le', 'me', 'ne', 'qe']\n"
     ]
    }
   ],
   "source": [
    "video_processor = VideoProcessor('Amharic_Sign_Language/train', 'Amharic_Sign_Language/train_labels.csv')\n",
    "X, y = video_processor.load_frames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "30\n",
      "X shape before flattening: (30, 224, 224, 3)\n",
      "Y shape before flattening: (30, 10)\n"
     ]
    }
   ],
   "source": [
    "print(len(X))\n",
    "print(len(y))\n",
    "print(\"X shape before flattening:\", X.shape)\n",
    "print(\"Y shape before flattening:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7s/step\n",
      "Feature shape (30, 2048)\n",
      "Labels shape (30, 10)\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = FeatureExtractor(video_processor.IMAGE_HEIGHT, video_processor.IMAGE_WIDTH)\n",
    "features = feature_extractor.extract_features(X)\n",
    "print(\"Feature shape\", features.shape)\n",
    "print(\"Labels shape\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deadstar/Documents/Amharic_Sign_Language/.venv/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - accuracy: 0.1319 - loss: 2.6409\n",
      "Epoch 2/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.2389 - loss: 2.0245\n",
      "Epoch 3/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.3056 - loss: 2.0500\n",
      "Epoch 4/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.5000 - loss: 1.5150\n",
      "Epoch 5/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.4375 - loss: 1.5029\n",
      "Epoch 6/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.5653 - loss: 1.3135\n",
      "Epoch 7/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.6722 - loss: 0.9793\n",
      "Epoch 8/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.7833 - loss: 0.6707\n",
      "Epoch 9/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.8472 - loss: 0.4742\n",
      "Epoch 10/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.7847 - loss: 0.5394\n",
      "Epoch 11/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8917 - loss: 0.3372\n",
      "Epoch 12/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.7833 - loss: 0.6222\n",
      "Epoch 13/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8486 - loss: 0.3287 \n",
      "Epoch 14/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.8694 - loss: 0.2196\n",
      "Epoch 15/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.9569 - loss: 0.0971\n",
      "Epoch 16/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.8694 - loss: 0.3021\n",
      "Epoch 17/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.9347 - loss: 0.1664 \n",
      "Epoch 18/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 1.0000 - loss: 0.0213\n",
      "Epoch 19/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.8694 - loss: 0.2258\n",
      "Epoch 20/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 1.0000 - loss: 0.0633\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7ffacd0265c0>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Input\n",
    "\n",
    "class CNNModel:\n",
    "    def __init__(self, num_classes, feature_size):\n",
    "        self.num_classes = num_classes\n",
    "        self.feature_size = feature_size\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential([\n",
    "            Dense(512, activation='relu', input_shape=(self.feature_size,)),\n",
    "            Dropout(0.5),\n",
    "            Dense(256, activation='relu'),\n",
    "            Dropout(0.3),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dense(self.num_classes, activation='softmax')\n",
    "        ])\n",
    "\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "# Create the model with the correct input shape\n",
    "cnn_model = CNNModel(10, features.shape[1])\n",
    "model = cnn_model.build_model()\n",
    "\n",
    "# Fit the model\n",
    "model.fit(features, y, epochs=20, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['che', 'gne', 'ha', 'he', 'hhe', 'ke', 'le', 'me', 'ne', 'qe']\n"
     ]
    }
   ],
   "source": [
    "# validation step \n",
    "video_processor = VideoProcessor('Amharic_Sign_Language/test', 'Amharic_Sign_Language/test_labels.csv')\n",
    "X, y = video_processor.load_frames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 4s/step\n",
      "Feature shape (439, 2048)\n",
      "Labels shape (439, 10)\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = FeatureExtractor(video_processor.IMAGE_HEIGHT, video_processor.IMAGE_WIDTH)\n",
    "features = feature_extractor.extract_features(X)\n",
    "print(\"Feature shape\", features.shape)\n",
    "print(\"Labels shape\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0180\n",
      "Test Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(features, y, batch_size=16)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "Number of misclassified samples: 0\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(features)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(y, axis=1)\n",
    "\n",
    "misclassified = np.where(y_pred_classes != y_true_classes)[0]\n",
    "print(f\"Number of misclassified samples: {len(misclassified)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common files: set()\n"
     ]
    }
   ],
   "source": [
    "train_filenames = set(pd.read_csv(\"Amharic_Sign_Language/train_labels.csv\")[\"filename\"])\n",
    "test_filenames = set(pd.read_csv(\"Amharic_Sign_Language/test_labels.csv\")[\"filename\"])\n",
    "print(\"Common files:\", train_filenames.intersection(test_filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,049,088</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │     \u001b[38;5;34m1,049,088\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,290\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,643,808</span> (13.90 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,643,808\u001b[0m (13.90 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,214,602</span> (4.63 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,214,602\u001b[0m (4.63 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,429,206</span> (9.27 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2,429,206\u001b[0m (9.27 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in ./.venv/lib/python3.10/site-packages (4.11.0.86)\n",
      "Requirement already satisfied: mediapipe in ./.venv/lib/python3.10/site-packages (0.10.21)\n",
      "Requirement already satisfied: numpy>=1.17.0 in ./.venv/lib/python3.10/site-packages (from opencv-python) (1.26.4)\n",
      "Requirement already satisfied: absl-py in ./.venv/lib/python3.10/site-packages (from mediapipe) (2.1.0)\n",
      "Requirement already satisfied: jax in ./.venv/lib/python3.10/site-packages (from mediapipe) (0.5.3)\n",
      "Requirement already satisfied: sentencepiece in ./.venv/lib/python3.10/site-packages (from mediapipe) (0.2.0)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in ./.venv/lib/python3.10/site-packages (from mediapipe) (4.25.6)\n",
      "Requirement already satisfied: jaxlib in ./.venv/lib/python3.10/site-packages (from mediapipe) (0.5.3)\n",
      "Requirement already satisfied: opencv-contrib-python in ./.venv/lib/python3.10/site-packages (from mediapipe) (4.11.0.86)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in ./.venv/lib/python3.10/site-packages (from mediapipe) (0.5.1)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.10/site-packages (from mediapipe) (3.10.1)\n",
      "Requirement already satisfied: attrs>=19.1.0 in ./.venv/lib/python3.10/site-packages (from mediapipe) (25.3.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in ./.venv/lib/python3.10/site-packages (from mediapipe) (25.1.24)\n",
      "Requirement already satisfied: CFFI>=1.0 in ./.venv/lib/python3.10/site-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
      "Requirement already satisfied: opt_einsum in ./.venv/lib/python3.10/site-packages (from jax->mediapipe) (3.4.0)\n",
      "Requirement already satisfied: ml_dtypes>=0.4.0 in ./.venv/lib/python3.10/site-packages (from jax->mediapipe) (0.4.1)\n",
      "Requirement already satisfied: scipy>=1.11.1 in ./.venv/lib/python3.10/site-packages (from jax->mediapipe) (1.15.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.10/site-packages (from matplotlib->mediapipe) (4.56.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.10/site-packages (from matplotlib->mediapipe) (1.3.1)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.10/site-packages (from matplotlib->mediapipe) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.10/site-packages (from matplotlib->mediapipe) (3.2.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.10/site-packages (from matplotlib->mediapipe) (1.4.8)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.10/site-packages (from matplotlib->mediapipe) (0.12.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.10/site-packages (from matplotlib->mediapipe) (24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.10/site-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
      "Requirement already satisfied: pycparser in ./.venv/lib/python3.10/site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install opencv-python mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[m\u001b[m\u001b[m\u001b[m"
     ]
    }
   ],
   "source": [
    "! wget -q https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@126.758] global loadsave.cpp:268 findDecoder imread_('./qe-test.jpg'): can't open/read file: check file path/integrity\n"
     ]
    }
   ],
   "source": [
    "img = cv2.imread('./qe-test.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not find label for image file le-test.jpeg\n",
      "Warning: Could not find label for image file qe-test.jpeg\n",
      "[-1]\n"
     ]
    }
   ],
   "source": [
    "video_processor = VideoProcessor('Amharic_Sign_Language/validation', 'Amharic_Sign_Language/test_labels.csv')\n",
    "X, y = video_processor.load_frames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = FeatureExtractor(video_processor.IMAGE_HEIGHT, video_processor.IMAGE_WIDTH)\n",
    "features = feature_extractor.extract_features(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.01506186, 0.09297454, 0.14822005, 0.03323105, 0.01668024,\n",
       "        0.00318067, 0.04579481, 0.6383235 , 0.00080061, 0.00573265],\n",
       "       [0.2015174 , 0.2953879 , 0.01305674, 0.40613452, 0.00110205,\n",
       "        0.01389759, 0.00177737, 0.05499005, 0.00272427, 0.00941203]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
